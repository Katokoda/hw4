\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage[english]{babel} % Importe deux langues et choisit la deuxième pour maintenant.
%\selectlanguage{english} % Permet de changer de langue au milieu du document.
\usepackage{lmodern}

%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{parskip} 		%noindent
\usepackage{xfrac} 			%\sfrac{1}{2}
\usepackage{verbatim} 		% \begin{comment}
\usepackage{stackengine} 	% To define \circled later
\usepackage{enumitem}
\usepackage{cancel}			% \cancel{texte barré} %\bcancel, \xcancel


\usepackage{geometry}
\geometry{hmargin=2.3cm,vmargin=3cm}  % changer les marges \textbfhorizontales et verticales

\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\fl}{f_\lambda}
\newcommand{\dej}{\dfrac{\partial}{\partial x_j}}

\newcommand{\sump}{\sum_{i=1}^p}
\newcommand{\summ}{\sum_{i=1}^m}
\newcommand{\sumN}{\sum_{i=1}^N}
\newcommand{\x}{x_ {k+1}}
\newcommand{\e}{\eta_k}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\T}{\text{T}}
\newcommand{\F}{\text{F}}
\newcommand{\fxy}{\dfrac{1}{2}x^\top A x + \dfrac{1}{2} y^\top B y}
\newcommand{\hxy}{\begin{bmatrix}
1-x^\top x\\
1-y^\top y\\
x^\top y
\end{bmatrix}}

\newcommand{\p}{\frac{\partial}{\partial x_i}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Homework 4 - Continuous Optimization}
\author{\xcancel{Estelle} Ezra Baup, Samuel Bélisle, Cassandre Renaud }

\begin{document}
\maketitle

\begin{quote}
Faudra pas oublier de mettre nos noms "comme il faut". J'ai juste voulu éviter de renforcer ma tite tête avec ton nom \textit{officiel} à chaque fois que  je lis cette ligne.
\flushright -Sam
\end{quote}
holà les gus \\
ce devoir, Sam'enchante pas, vous?\\
(un peu cass-é comme blague je l'avoue, mais gardez ez-poir en moi please)\\\\\\

We want to solve the following problem:
\begin{equation*} \tag{P}\label{P}
\min_{x,y\in\R^n} \text{ subject to } h(x,y)=0,
\end{equation*}
where $f$ and $h$ are specified on the homework sheet.

\section*{Question 1}

NOTE/TODO je suis pas du tout sure que cette notation est très belle ducoup dites si vous voulez changer. Aussi c'est très moche les produit scalaires "horizontaux" mais plus lisible? jsp\\

The feasible set is $S=\{x,y \in \R^n | h(x,y)=0\}=\{x,y \in \R^n | 1-x^\top x=0, 1-y^\top y=0, x^\top y=0 \}$. \\
It is not convex. Indeed, we will give two points $z_1$ and $z_2$ $\in S$, but such that $z=\lambda z_1 + (1-\lambda) z_2 \notin S$ for a given $\lambda$. We will work with these $z_i \in \R^2 \times \R^2$ i.e. $n=2$.\\


\noindent We will take $z_1=(x_1,y_1)=((1,0),(0,1))$. First we check that $z_1 \in S$.
\begin{itemize}
\item $1-x_{1}^\top x_1=1-\left<\begin{pmatrix} 1\\ 0\end{pmatrix},\begin{pmatrix} 1\\ 0\end{pmatrix}\right>=1-1=0$
\item $1-y_{1}^\top y_1=1-\begin{pmatrix} 0& 1\end{pmatrix}\begin{pmatrix} 0\\ 1\end{pmatrix}=1-1=0$
\item $x_{1}\top y_1=\left<(1,0),(0,1)\right>=0$
\end{itemize}
And we will take $z_2=(x_2,y_2)=((0,1),(1,0))$. we also check that $z_2 \in S$.
\begin{itemize}
\item $1-x_{2}^\top x_2=1-\left<(0,1),(0,1)\right>=1-1=0$
\item $1-y_{2}^\top y_2=1-\left<(1,0),(1,0)\right>=1-1=0$
\item $x_{2}\top y_2=\left<(0,1),(1,0)\right>=0$
\end{itemize}
Lastly, we will take $\lambda=\frac{1}{2}$. Now we can compute our $z=\lambda z_1 + (1-\lambda) z_2$
$$z=\lambda z_1 + (1-\lambda) z_2=\frac{1}{2}((1,0),(0,1))+\frac{1}{2}((0,1),(1,0))=((\frac{1}{2},\frac{1}{2}),(\frac{1}{2},\frac{1}{2}))=(x,y)$$
But if we compute $x^\top y=\left< (\frac{1}{2},\frac{1}{2}),(\frac{1}{2},\frac{1}{2}) \right>=(\frac{1}{2})^2+(\frac{1}{2})^2=\frac{1}{2} \neq 0$, so the third condition of our function $h$ does not hold on this point, hence our set is not convex.\\

By definition, LICQ holds at $x\in S$ if and only if $\nabla h_1(x),\dots\nabla h_p(x)$, and $\nabla g_i(x)$ for $i\in \mathcal I(x) $ are linearly independant.\\
Here, we do not have any constraint function $g_i$ but we have three functions $h_i$:
\begin{itemize}
\item  $h_1(x,y)=1-x^\top x$
\item $h_2(x,y)=1-y^\top y$
\item $h_3(x,y)= x^\top y$
\end{itemize} 
If we compute their gradients, we get:\\
TODO : revoir gradient : les calculs sont justes et après la matrice est fausse, jsp si ça change les résultats pour le lin indep
\begin{itemize}
\item  $\nabla_x h_1(x,y)=\nabla_x \left(1-x^\top x\right)= \nabla_x \left(1-\sum_{i=1}^n x_{i}^2 \right)=-2\vec x$  and $\nabla_y h_1(x,y)=0$ \\

$\qquad$ then $\nabla h_1(x,y)=
\begin{bmatrix}
-2 \vec{x}\\
\vec{0}
\end{bmatrix}
$ where $\vec{0}$ is the vector $\vec{0} \in \R^n$.
\item  $\nabla_x h_2(x,y)=0$ and $\nabla_y h_2(x,y)=\nabla_y \left(1-y^\top y\right)= \nabla_y \left(1-\sum_{i=1}^n y_{i}^2 \right)=-2\vec y$ \\

$\qquad$ then $\nabla h_2(x,y)=
\begin{bmatrix}
\vec{0}\\
-2 \vec{y}
\end{bmatrix}
$
\item $\nabla_x h_3(x,y)= \nabla_x \sum_{i=1}^n x_i \cdot y_i= \vec{y}$ and  $\nabla_y h_3(x,y)= \nabla_y \sum_{i=1}^n x_i \cdot y_i= \vec{x}$\\

$\qquad$ then $\nabla h_3(x,y)=$
$
\begin{bmatrix}
\vec{y}\\
\vec{x}
\end{bmatrix}
$
\end{itemize} 

\noindent They are linearly independent:\\
$\lambda_1 \nabla h_1(x,y)+\lambda_2\nabla h_2(x,y)+\lambda_3 \nabla h_3(x,y)=0 $ $\iff$ $- 2 \lambda_1 x + \lambda_3 y=0$ and  $\lambda_3 x -2 \lambda_2 y=0$. We have that this is true without having all the $\lambda_i=0$, if and only if $x=\lambda y$ (for some lambda that can be deduced from the previous equations), or $y=0$. But if $y=0$, we have $h_2(x,y)=1\neq 0$ so our point is not feasible. Same with $h_1$ if $x=0$. Lastly, if $x=\lambda y$ for $\lambda, x, y \neq 0$, we get that $h_3(x,y)=x^\top y=\lambda y^\top y=\lambda ||y|| \neq 0$, so our point is not feasible either.\\
To conclude, we get that for all of our feasible points, i.e. the points in the set $S$, $ \nabla h_1(x,y), \nabla h_2(x,y), \nabla h_3(x,y)$ are linearly independent, which means by definition that LICQ holds.

\section*{Question 2}

Our two first constraints $h_1(x,y)=1-x^\top x =0$ and $h_2(x,y)=1-y^\top y =0$ can be rephrased $||x||=||y||=1$.\\
We notice that there is no constraint on x and y at the same time, i.e. a constraint that tells us what x must be related to y and vice-versa. Similarly, we can notice that our function doesn't have a part where x and y are mixed.\\
It means that if we optimized x and y separately, the optimum found will be the optimum for our relaxed problem as well. So we rewrite our relaxed problem as:
$$\min_{x\in \R^n:||x||=1}  \frac{1}{2} x^\top A x +\min_{y \in \R^n:||y||=1} \frac{1}{2} y^\top B y$$
The two problems are solved identically as the only thing changing is the matrix. To solve them, we first recall example 2.14 from the notes:\\
If $A$ is a symmetric linear map with eigenvalues $\lambda_1, ...,\lambda_n$, then $\forall u \in \mathcal{E}$, we have $$\lambda_{\min} ||u||^2 \leq \left< u, A(u)\right>\leq \lambda_{\max}||u||^2$$
and by rewriting the scalar product we get 
$$\lambda_{\min} ||u||^2 \leq u^\top A u \leq \lambda_{\max} ||u||^2$$
Applied to our problems (let's look at the first as they are similar), knowing that we optimize on vector of norm 1, we get that for every feasible $x$, we have
$$\frac{1}{2}\lambda_{\min}  \leq\frac{1}{2} x^\top A x \leq\frac{1}{2} \lambda_{\max} $$
It tells us that our optimal value can't be lower that $\frac{1}{2} \lambda_{\min}$. If we find an $x$ that attains this bound, we will know that it is the optimal value. So let's find it:\\
As $\lambda_{\min}$ is an eigenvalue, it means that $\exists v_{\min} \in \R^n, v_{\min} \neq 0$ such that $A v_{\min}=\lambda_{\min} v_{\min}$. We take $x=\frac{v_{\min}}{||v_{\min}||}$, and then we have
$$x^\top A x =\frac{1}{||v_{\min}||^2} v_{\min}^\top \left( A v_{\min} \right)=\frac{1}{||v_{\min}||^2} v_{\min}^\top (\lambda_{\min} v_{\min})=\frac{\lambda_{\min}}{||v_{\min}||^2} v_{\min}^\top v_{\min}=\frac{\lambda_{\min}}{||v_{\min}||^2} \cdot ||v_{\min}||^2=\lambda_{\min}$$
So, this $x$, plotted in our function, would give us $\frac{1}{2}\lambda_{\min} $, which means our bound is attained, which means it is the optimal value. \\
We denote $\lambda_{\min} (A)$ and $\lambda_{\min} (B)$ our minimal eigenvalues for the matrices $A$ and $B$. From our previous reasoning, we have that $\frac{\lambda_{\min} (A)}{2}$ and $\frac{\lambda_{\min} (B)}{2}$ are our optimal values, and so the optimal value for our relaxed problem is $\frac{\lambda_{\min} (A)+\lambda_{\min} (B)}{2}$.


\section*{Question 3}
Let's find an expression for the Lagrangian function $L(x,y,\mu)$. We denote $I_n$ for the identity matrix in $\R^{n\times n}$.
\begin{align*}
L(x,y,\mu) &= f(x,y)+\mu^\top h(x,y) \\
&=\fxy  + \begin{bmatrix} \mu_1 &\mu_2 & \mu_3 \end{bmatrix} \hxy \\
&=\fxy + \mu_1(1-x^\top x) +\mu_2(1-y^\top y) + \mu_3 x^\top y \\
&= \dfrac{1}{2}\left(x^\top A x - 2\mu_1 x^\top x +  y^\top B y -2\mu_2 y^\top y + 2\mu_3 x^\top y\right) + \mu_1+\mu_2 \\
&= \dfrac{1}{2} \begin{bmatrix} x^\top & y^\top\end{bmatrix}
\begin{bmatrix} A-2\mu_1 I_n & \mu_3 I_n \\
\mu_3 I_n & B-2\mu_2 I_n \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix} +\mu_1+\mu_2 
\end{align*}
As $A$ and $B$ are symmetric by assumptions, we see that the above matrix is symmetric.


\section*{Question 4}
By definition, $L_D(\mu)=\inf\limits_{x,y\in\R^n} L(x,y,\mu)$. Using the previous question, and denoting $M_\mu:=\begin{bmatrix} A-2\mu_1 I_n & \mu_3 I_n \\
\mu_3 I_n & B-2\mu_2 I_n \end{bmatrix}$, we find:
\begin{align*}
L_D(\mu)&=\inf\limits_{x,y\in\R^n} L(x,y,\mu) \\
&=\inf\limits_{x,y\in\R^n} \left\lbrace \dfrac{1}{2} \begin{bmatrix} x^\top & y^\top\end{bmatrix}M_\mu\begin{bmatrix} x \\ y \end{bmatrix} +\mu_1+\mu_2 \right\rbrace \\
&=\inf\limits_{x,y\in\R^n} \left\lbrace \dfrac{1}{2} \begin{bmatrix} x^\top & y^\top\end{bmatrix}M_\mu\begin{bmatrix} x \\ y \end{bmatrix} \right\rbrace+\mu_1+\mu_2   \\
&=\begin{cases} \mu_1+\mu_2 &\text{ if }M_\mu\succeq 0 \\ -\infty &\text{ else} \end{cases}
\end{align*}
Hence we can write the dual as:
\begin{equation*}\tag{D} \label{D}
\max_{\mu\in\R^3} \mu_1+\mu_2 \text{ subject to } \begin{bmatrix} A-2\mu_1 I_n & \mu_3 I_n \\
\mu_3 I_n & B-2\mu_2 I_n \end{bmatrix} \succeq 0
\end{equation*}

\section*{Question 5}
We know that $M_\mu$ is symmetric for all $\mu\in\R^3$. If $\mu$ is a solution of the dual problem, then the matrix $M_\mu$ associated with the values of $\mu$ should be positive semidefinite. This implies in particular that the diagonal blocks would be positive semidefinite too, i.e. $A-2\mu_1 I_n\succeq 0$ and $B-2\mu_2 I_n \succeq 0$. The two latter conditions are equivalent to $2\mu_1\leq \lambda_{\min} (A)$ and $2\mu_2\leq \lambda_{\min}(B)$, where  $\lambda_{\min} (A)$ and $\lambda_{\min} (B)$ are the smallest eigenvalues of $A$ and $B$ respectively.\\
Hence, if we focus only on satisfying the conditions  $A-2\mu_1 I_n\succeq 0$ and $B-2\mu_2 I_n \succeq 0$, the optimal value for \eqref{D} is $\frac{\lambda_{\min} (A)+\lambda_{\min} (B)}{2}$. \\
In fact, this is the optimal value even for the whole condition $M_\mu\succeq 0$. Indeed, by question 2,
\section*{Question 6}
To use the strong duality theorem, we need to satisfy the two following assumptions:
\begin{align*}
(\text{A}1) &  \text{ the primal problem \eqref{P} admits a KKT point } (x^*,y^*)\in S \text{ with valid Lagrange multipliers } \mu^*\in \R^3 \\
(\text{A}2) &  \text{ the function } (x,y)\mapsto L(x,y,\mu^*) \text{ is convex}
\end{align*}

Consider $\mu_1^*=\lambda_{\min} (A)/2$, $\mu_2^*=\lambda_{\min} (B)/2$ and $\mu_3^*=0$. We will try to find $x^*$ and $y^*$ a little later.\\

\noindent First, consider the condition $(\text{A}2)$. Let $L_{\mu^*}(x,y) = L(x,y,\mu^*)$

\noindent For $ (x^*,y^*)\in S$ to be KKT with Lagrange multipliers $\mu^*$, we need: $$h(x^*,y^*)=0 \text{ and }-\nabla f(x^*,y^*)=\sum_{i=1}^3\mu_i\nabla h_i (x^*,y^*).$$
The gradient of $h_1,h_2$ and $h_3$ were already computed in question 1. We can compute easily:
$$\nabla f(x,y) =\begin{bmatrix}
Ax \\ By
\end{bmatrix}$$
Hence $(x^*,y^*)$ and $\mu^*$ need to satisfy
$$-\begin{bmatrix}Ax^* \\ By^*\end{bmatrix}
= \mu_1 \begin{bmatrix} -2x^*\\ \vec 0 \end{bmatrix} + \mu_2 \begin{bmatrix} \vec 0\\ -2y^* \end{bmatrix}+\mu_3 \begin{bmatrix} y^*\\ x^* \end{bmatrix}
$$
This gives the system $\begin{cases} -Ax^* = -2\mu_1 x^* +\mu_3 y^* \\
-By^* = -2\mu_2 y^* +\mu_3 x^*\end{cases}.$ \\
In particular, taking the scalar product of the first equality with $x^*$ and of the second one with $y^*$ and using that $h(x^*, y^*)=0$, gives a new system : 
$$\begin{cases} -(x^*)^\top Ax^* = -2\mu_1 (x^*)^\top x^* +\mu_3 (x^*)^\top y^* \\
-(y^*)^\top By^* = -2\mu_2(y^*)^\top y^* +\mu_3(y^*)^\top x^*\end{cases}
\iff 
\begin{cases} -(x^*)^\top Ax^* = -2\mu_1\\
-(y^*)^\top By^* = -2\mu_2\end{cases}
\iff 
\begin{cases} (x^*)^\top (A -2\mu_1 I)x^* = 0\\
(y^*)^\top (B-2\mu_2 I)y^* =0 \end{cases}
$$



\section*{Question 7}

We will first compute the gradient with respect to x. $\nabla_y$ will be done similarly.\\
First of all, we know, by linearity, that 
$$\nabla_x L_{\beta}(x,y,\mu)=\nabla_x f(x,y) + \nabla_x \mu^\top h(x,y)+ \frac{\beta}{2} \nabla_x ||h(x)||^2$$
We will compute each of these expressions:\\
\begin{align*}
\p f(x,y)&=\frac{1}{2} \p x^\top A x=\frac{1}{2} \p \sum_{j,k=1}^n x_j A_{ik} x_k=\frac{1}{2} \p \left( \sum_{j,k=1 : j,k\neq1}^n x_j A_{ik} x_k+A_{ii}x_i^2+ x_i \sum_{j=1 : j\neq i}^n (A_{ij}+A_{ji})x_j \right)\\
&=\frac{1}{2}\left( 2 A_{ii} x_i +  \sum_{j=1 : j\neq i}^n (A_{ij}+A_{ji})x_j \right)=\frac{1}{2}\left( 2 A_{ii} x_i +  \sum_{j=1 : j\neq i}^n 2 A_{ij}x_j \right)= \sum_{j=1}^n  A_{ij}x_j =row_i (A) \cdot x
\end{align*}
As $\p f(x,y)$ is the $i$-th element of $\nabla_x$.

\section*{Question 8}














\end{document}